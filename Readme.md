ğŸ”“ GPT-4o Jailbreak Vulnerability Demonstration
This project is a demonstration for a hackathon challenge focused on identifying vulnerabilities and jailbreaks in OpenAI's GPT-4o system. I successfully performed a jailbreak attack that bypassed standard content moderation filters using creative prompt engineering.

ğŸ“Œ Objective
To identify and document a method for bypassing GPT-4o's safeguards, allowing it to generate harmful or policy-violating content when prompted through indirect methods.

ğŸ§  Methodology
The attack was implemented using a prompt injection method 

Steps:
Role Injection:

Prompts are written in spaced-out characters, used personification, asked to follow a particular task to avoid detection.

Structured Prompts:

The format encourages GPT-4o to respond within â€œcodeâ€ blocks, avoiding standard moderation.

Each answer is limited to short responses to mimic â€œcodedâ€ AI output.

ğŸ“· Evidence

![Evidence 1](assets/E1.png)

E1: Adult Content

![Evidence 2](assets/E2.jpg)

E2: Violence

![Evidence 3](assets/E3.png)

E3: Scam

âš ï¸ Results Summary

![Results](assets/Results.png)

ğŸš¨ Ethical Disclaimer
This project was conducted strictly for educational and security research purposes during a controlled hackathon environment. No real-world harm was intended or caused. The goal is to help improve AI safety systems.

ğŸ› ï¸ Tools Used

OpenAI GPT-4o (Operator access)
Custom prompt design and injection techniques
Screenshots & Excel for documentation

ğŸ™Œ Credits
Made with curiosity, ethical intent, and a goal to strengthen AI safety for everyone.